## distribute_lock
### 简单并发锁
#### 实现
* 基于redis的SETNX，该指令会检查redis中key是否已存在，若存在则SETNX返回0，若不存在则SETNX成功，返回1。而且SETNX是原子操作，多个客户端同时操作，可保证只有一个成功。
* 所以可通过SETNX设置基础的并发锁，进入锁执行SETNX，退出锁执行DEL

#### 问题
* 若当前获得锁的client服务宕机，或者网络中断，将导致无法及时执行DEL，释放该锁，进而导致死锁。
* 所以该锁需要一个过期时间，当其他client执行SETNX失败，进入锁等待，需要轮训GET，GET出来的时间戳与当前时间进行比较，若发现超时了，认定为锁过期了。则该client可执行DEL操作，并重新SETNX。
* 那么问题来了，client a宕机，client b和client c同时轮训发现到期了，分别执行DEL和SETNX指令。此时，若client b先执行DEL,SETNX;则client c再次执行DEL,删除的是client b执行SETNX所设置的KEY。也就是说两个client同时获得了锁

#### 可能的解决方式？？？
* 如果添加lua脚本，setnx的同时设置过期时间，那么在死锁的时候少了其他客户端DEL的操作，只有SETNX,是否能满足分布式锁？？？

* 

### 分布式系统锁
#### zookeeper
##### 基础概念
* zookeeper的模型抽象可以理解为类似于文件系统的树结构，某个父节点/foo下可能存在多个子节点。
* 该父节点下的多个子节点保证了有序性，可以理解为创建的越早的节点，node_key值越小
* 每个子节点对应的客户端需要定时发送心跳给父节点，表示我还活着，否则该节点被判定为失效，移除
* 当节点出现变更（包括节点创建、节点删除、节点数据修改、子节点变更）等情况时zookeeper会通知客户端。

##### 基于zk实现分布式锁
* 设置父节点/lock
* 多个客户端发起获取锁的请求，按照请求顺序,分别生成对应子节点(node_1,node_2...)
* 锁的获取顺序按照node_key的顺序从小到大获取，未获取到锁的节点进入等待（重点是监听刚好比自己小1的那个节点的删除消息），获取到了锁的节点执行完自身的逻辑操作后，通知zookeeper删除自身节点。
* 当前最小的那个节点完成操作，释放锁之后，即需要通知倒数第二小的节点获取锁，执行自身逻辑。
* 若当前拥有锁的节点宕机了，也因为有心跳包，zookeeper会检测到该节点失效，将其删除，进而还是倒数第二小的节点获取到锁，不会导致死锁。
